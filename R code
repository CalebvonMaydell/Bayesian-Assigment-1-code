qbeta(.5, 3,14)
#0.1636542

#To get the coefficitent I wanted to a few things, so I will develop a product function to make this easier, this will also allow for easier multiplication later of the polynomials

product <- function(vec){   #multiply all elements of a matrix
    out <- 1
    for(i in 1:length(vec)){
         out <- out*vec[i]
    }
    out
}

c<-product(279:293)




#3.7 d)
n <- 278
p <- 2/15

x_values <- 0:n


probabilities <- dbinom(x_values, size = n, prob = p)

barplot(probabilities, col = "skyblue", main = "Binomial Distribution",
        xlab = "Number of Successes", ylab = "Probability", names.arg = NULL)

axis(side = 1, at = seq(0, n, by = 10), labels = seq(0, n, by = 10))



#4.8
menwbach <- c(1, 0, 0, 1, 2, 2, 1, 5, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 3, 2, 0, 0, 3, 0, 0, 0, 2, 1, 0, 2, 1)  # used chat gpt to add the commas for this and next line
mennobach<- c(1, 0, 0, 1, 2, 2, 1, 5, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 3, 2, 0, 0, 3, 0, 0, 0, 2, 1, 0, 2, 1,  
              2, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 0, 2, 2, 0, 2, 1, 0, 0, 3, 6, 1, 6, 4, 0, 3, 2, 0, 1, 0, 0, 0, 3, 0,
              0, 0, 0, 0, 1, 0, 4, 2, 1, 0, 0, 1, 0, 3, 2, 5, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 2, 0, 2, 4, 1, 1, 1,
              2, 0, 1, 1, 1, 1, 0, 2, 3, 2, 0, 2, 1, 3, 1, 3, 2, 2, 3, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 3, 3, 0, 1, 2, 2, 2,
              0, 6, 0, 0, 0, 2, 0, 1, 1, 1, 3, 3, 2, 1, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 2, 4, 1, 2, 3, 2, 0, 0,
              0, 1, 0, 0, 1, 5, 2, 1, 3, 2, 0, 2, 1, 1, 3, 0, 5, 0, 0, 2, 4, 3, 4, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 1,
              1, 0, 2, 1, 3, 3, 2, 2, 0, 0, 2, 3, 2, 4, 3, 3, 4, 0, 3, 0, 1, 0, 1, 2, 3, 4, 1, 2, 6, 2, 1, 2, 2)

sum(menwbach)  # 36
length(menwbach) #n = 38

length(mennobach) #n = 256
sum(mennobach) #341

set.seed(1234)
sampleswith <- rgamma(5000, shape = 38, rate = 39)
hist(sampleswith)
mean(sampleswith)

sampleswithout<-rgamma(5000, shape = 343, rate = 257)
hist(sampleswithout)

diff = sampleswithout - sampleswith

mean(diff)
sd(diff)
quantile(diff, 0.95)
quantile(diff, 0.05)

# Set a seed for reproducibility
set.seed(42)

# Generate a sample data
data_sample <- rpois(1000, lambda = 1.4)

# Plot the histogram
hist(data_sample, breaks = seq(-0.5, max(data_sample) + 0.5, by = 1), probability = TRUE,
     main = 'Histogram of Data Sample and Poisson Distribution',
     xlab = 'Value', ylab = 'Probability', col = 'lightblue', border = 'black')

# Overlay the Poisson distribution with lambda = 1.4
x_values <- 0:max(data_sample)
poisson_dist <- dpois(x_values, lambda = 1.4)
lines(x_values, poisson_dist, col = 'red', type = 'h', lwd = 2)

# Add a legend
legend('topright', legend = c('Data Sample', 'Poisson (Î»=1.4)'),
       fill = c('lightblue', 'red'), border = 'black')



num_samples <- 218

result_matrix <- matrix(0, ncol = 2, nrow = length(sampleswithout))

# Generate samples and count zeros and ones for each
for (i in 1:length(sampleswithout)) {
  poisson_params <- sampleswithout[i]
  sample <- rpois(218, lambda = poisson_params)  
  
  # Count zeros and ones
  num_zeros <- sum(sample == 0)
  num_ones <- sum(sample == 1)
  
  # Store the counts in the matrix
  result_matrix[i, ] <- c(num_zeros, num_ones)
}


sum(mennobach == 0)   #91
sum(mennobach == 1)   #60

# Assuming result_matrix is already defined
smoothScatter(result_matrix[,1], result_matrix[,2], main = "Scatter Plot with Density", xlab = "Zeros", ylab = "Ones", col = "blue", pch = 16)


smoothScatter(result_matrix[,1], result_matrix[,2], main = "Scatter Plot with Density", xlab = "Zeros", ylab = "Ones", col = "blue", pch = 16, xlim = c(0, 100), ylim = c(0, 100))

# Highlight the data 
points(91, 60, col = "red", pch = 16)


#5.2
m = 1:50   #"prior sample size"

mua = (m*75+1203.2)/(m+16)   #posterior mean for a
mub = mub = (m*75+1240)/(m+16) #posterior mean for b

sa =     1/(m+16)*(100*m+799.35 + 0.64*m/(m+16))   #posterior variance for a
sb =    1/(m+16)*(100*m+984.15 + 100*m/(m+16))    #posterior variance for b

# Assuming you have a vector m with length n
n <- length(m)

# Initialize palb as a vector of zeros or with appropriate dimensions
palb <- numeric(n)


for (i in 1:n) {
    if (sa[i] > 0 && sb[i] > 0) {
        samplea <- rnorm(1000, mua[i], sa[i])
        sampleb <- rnorm(1000, mub[i], sb[i])
        palb[i] <- mean(samplea < sampleb)
    } else {
        # Handle the case where standard deviations are zero, from chat gpt to fix an error
        palb[i] <- NA
    }
}

plot(m,palb)
summary(lm(palb~m)) #linear model will show if theres a linear trend (mostly so I can justify if theres a relation)

# 6.1
agam<-128
bgam<-agam
ath <-2
bth<-1

#number of trials and sums
na = 38
nb = 256
sa = 36
sb = 341

S = 5000 #no of trials

#set initial values
gamman<-rgamma(1,agam,bgam)
thetan<-rgamma(1,ath,bth)

set.seed(123)
for (i in 1:S){
    thetan<-rgamma(1,sa+sb+ath, na+gamman*nb+bth)
    gamman<-rgamma(1,sb+agam, thetan*nb + bgam)
}

thetan
gamman
#repeat with each initial value given

thb = thetan*gamman
tha = thetan

tha-thb




#7.5

interexp <- read.csv("C:/Users/Caleb/OneDrive/Desktop/temp/interexp.csv", sep="")  #adjust to your file location
View(interexp)

remove<-na.omit(interexp)

mean(remove$yA)
mean(remove$yB)
var(remove$yA)
var(remove$yB)

#since the first 26 rows are the useful ones for correlation
fulldata<-interexp[1:26,]
cor(fulldata$yA, fulldata$yB)





test<-interexp #I used a test frame so I didnt change the original while testing incremental changes and its annoying to change back
#replacement function
# Check if any NA values in the second column (yB)
if (any(is.na(test$yB))) {
  # Replace NA values in the second column (yB) with the calculated values
  test[is.na(test$yB), "yB"] <- 24.76385 + (test[is.na(test$yB), "yA"] - 24.14115) * 0.6164509 * sqrt(5.321267/4.864169)
}

if (any(is.na(test$yA))) {
  # Replace NA values in the first column (yB) with the calculated values
  test[is.na(test$yA), "yA"] <- 24.14115 + (test[is.na(test$yA), "yB"] - 24.76385) * 0.6164509 * sqrt(4.864169/5.321267)
}

test  #should have full values
t.test(test$yA, test$yB, paired = TRUE)   # t test for paired 



#part c

library(mvtnorm)

#input the data values for the initial beliefs
theta_A = 24.14115
theta_B = 24.76385
sigma_squared_A = 5.321267
sigma_squared_B = 4.864169
rho = 0.6164509

mu0 <-c(theta_A, theta_B)
L0 <-matrix(c(5.32, 3.15, 3.15, 4.86), nrow=2, ncol =2)


mun=mu0
Lam0 = L0

#Similar to the slides, we onl have a loose value of this belief
nu0<-4
S0<-matrix(c(5.32, 3.15, 3.15, 4.86), nrow=2, ncol =2)
Sigma<-matrix(c(5.32, 3.15, 3.15, 4.86), nrow=2, ncol =2)
n<-dim(test)[1] ; ybar<-apply(test,2,mean)
THETA<-SIGMA<-NULL
YS<-NULL

set.seed(123123)

for (s in 1:5000)
{
    ###update the thetas
    Ln<-solve(solve(L0)+n*solve(Sigma))
    mun<-Ln%*%(solve(L0)%*%mu0 + n*solve(Sigma)%*%ybar)
    theta<-rmvnorm(1,mun,Ln)


    ###Sigma update
     Sn<-S0+(t(test)-c(theta) )%*%t( t(test)-c(theta))
     Sigma<-solve(rWishart(1,nu0+n, solve(Sn)))
    
    YS<-rbind(YS, theta, Sigma)

    #save results
    THETA<-rbind(THETA,theta)
    SIGMA<-rbind(SIGMA, c(Sigma))

    cat(s,round(theta,2), round(c(Sigma,2), "\n")
}

quantile(SIGMA[,2]/sqrt(SIGMA[,1]*SIGMA[,4]), c(0.025, 0.975))

quantile(THETA[,2]-(THETA[,1]), c(0.025, 0.975))

#q7 

set.seed(123123)
p_binomial <- 0.35 
sample_size <- 10
num_samples <- 500

binomial_data <- rbinom(num_samples, size = sample_size, prob = p_binomial)

posterior_alpha <- binomial_data + 20
lowci <- qbeta(0.1, posterior_alpha,30)
upperci<-qbeta(0.9, posterior_alpha,30)


sum(lowci <  p_binomial & p_binomial< upperci)/500

